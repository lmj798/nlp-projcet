{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['content', 'personality'],\n",
      "    num_rows: 58972\n",
      "})\n",
      "{'content': \"OH!The shame!It's unbearable.:P|||Aha...Well if it weren't for my superiority complex,I'd probably be down on the ground,all neurotic and depressive,locked up in an asylum.You need it in order to develop past your initial sphere of...|||Well excuse me for being such a drag.If it truly were a superiority complex,then I wouldn't be curious about the opinion of other people when it comes to understanding them.I just stated above that...|||I consider myself to be rather complicated.The mbti,jung's studies,psychology in general and philosophy inspired me to reach a certain conclusion related to people:if some are dumb,then it's not...|||I sometimes do simulate a large variety of things in my head resulting in intense feelings close to what I'd experience in reality.It's a bit complicated for me because while some people believe...|||I can't say I hate people.I always had trouble hating because I usually find explanations for the stupidity of people and mostly understand the fact that they have insecurities,social issues and...|||Why not trust science?It sometimes is indeed wrong,but that doesn't mean you mustn't give it a try.It means there are a couple of new things that need to be perfected.Science means finding a logic...|||Thanks for the offer,I'll think about it:laughing: who sais I can't marry?My idealism?a bit too soon tough...:P Not idealistic actually.I'm sure these toughts and theories can be put into practice...|||Thank you for the insight.I'm not a product of such an environment,so I don't fully understand the feeling of growing up in such a place. That's a good enough reason for all my arguements to be...|||\", 'personality': 'INTP'}\n",
      "just watched the american in line in front of me search \"barking ramz remix\" on apple music|||@user that one's hot summer, hot hot summer|||girls aloud were right, it's gonna be a long hot summer|||for those that don't know i produce wank pop here you go link|||late to party but that spoken word lipsync was great|||lady in subway asked for \"chipottle\" sauce|||hey they gotta isaac emoji ‍|||boyfriend said \"girl in front of fire\" and meant this pic link|||@user my tweet refers exclusively to abortion and not sterilisation tho, i feel that this is a separate t… link|||what i mean to say is it's explicitly a women's rights issue - they are trying to regain total control over women's… link|||yes some men can get pregnant but restricting abortions is an act of oppression targeted at women specifically, and… link|||purchased hatsune miku game see you in a month|||enough link|||just said to myself \"fishy fingers for the girly wirlies\". i have a 39 degree fever|||will never understand people who get alarmed that we don't teach kids the same science they learned at school decad… link|||currently the sexiest man with tonsillitis alive|||@user just be a normal lollipop like :(|||THEY PISS ME OFF SO BAD link|||hairline receding, abs disappearing, starting an IT support job. successful transition from female to just some guy|||do! do you! got a first! aid! kit! handy! link|||do u ever deep how barbie girl came out in 1997 like how ahead of their time were theyyy|||@user_ we wose|||ditch pronouns shirts i want indefinite articles merch|||lost my mind at this plus the fact no one ironed it link|||\n",
      "['just', 'watched', 'the', 'american', 'in', 'line', 'in', 'front', 'of', 'me', 'search', 'barking', 'ramz', 'remix', 'on', 'apple', 'music', 'user', 'that', \"one's\", 'hot', 'summer', 'hot', 'hot', 'summer', 'girls', 'aloud', 'were', 'right', \"it's\", 'gonna', 'be', 'a', 'long', 'hot', 'summer', 'for', 'those', 'that', \"don't\", 'know', 'i', 'produce', 'wank', 'pop', 'here', 'you', 'go', 'link', 'late', 'to', 'party', 'but', 'that', 'spoken', 'word', 'lipsync', 'was', 'great', 'lady', 'in', 'subway', 'asked', 'for', 'chipottle', 'sauce', 'hey', 'they', 'gotta', 'isaac', 'emoji', '\\u200d', 'boyfriend', 'said', 'girl', 'in', 'front', 'of', 'fire', 'and', 'meant', 'this', 'pic', 'link', 'user', 'my', 'tweet', 'refers', 'exclusively', 'to', 'abortion', 'and', 'not', 'sterilisation', 'tho', 'i', 'feel', 'that', 'this', 'is', 'a', 'separate', 't…', 'link', 'what', 'i', 'mean', 'to', 'say', 'is', \"it's\", 'explicitly', 'a', \"women's\", 'rights', 'issue', '-', 'they', 'are', 'trying', 'to', 'regain', 'total', 'control', 'over', \"women's…\", 'link', 'yes', 'some', 'men', 'can', 'get', 'pregnant', 'but', 'restricting', 'abortions', 'is', 'an', 'act', 'of', 'oppression', 'targeted', 'at', 'women', 'specifically', 'and…', 'link', 'purchased', 'hatsune', 'miku', 'game', 'see', 'you', 'in', 'a', 'month', 'enough', 'link', 'just', 'said', 'to', 'myself', 'fishy', 'fingers', 'for', 'the', 'girly', 'wirlies', 'i', 'have', 'a', '39', 'degree', 'fever', 'will', 'never', 'understand', 'people', 'who', 'get', 'alarmed', 'that', 'we', \"don't\", 'teach', 'kids', 'the', 'same', 'science', 'they', 'learned', 'at', 'school', 'decad…', 'link', 'currently', 'the', 'sexiest', 'man', 'with', 'tonsillitis', 'alive', 'user', 'just', 'be', 'a', 'normal', 'lollipop', 'like', '(', 'they', 'piss', 'me', 'off', 'so', 'bad', 'link', 'hairline', 'receding', 'abs', 'disappearing', 'starting', 'an', 'it', 'support', 'job', 'successful', 'transition', 'from', 'female', 'to', 'just', 'some', 'guy', 'do', 'do', 'you', 'got', 'a', 'first', 'aid', 'kit', 'handy', 'link', 'do', 'u', 'ever', 'deep', 'how', 'barbie', 'girl', 'came', 'out', 'in', '1997', 'like', 'how', 'ahead', 'of', 'their', 'time', 'were', 'theyyy', 'user_', 'we', 'wose', 'ditch', 'pronouns', 'shirts', 'i', 'want', 'indefinite', 'articles', 'merch', 'lost', 'my', 'mind', 'at', 'this', 'plus', 'the', 'fact', 'no', 'one', 'ironed', 'it', 'link']\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "train_set = datasets.load_from_disk(dataset_path=\"./train\")\n",
    "print(train_set)\n",
    "test_set = datasets.load_from_disk(dataset_path=\"./valid\")\n",
    "print(test_set[0])\n",
    "\n",
    "word_train = []\n",
    "y_train = []\n",
    "for i in range(len(train_set)):\n",
    "    word_train.append(train_set[i][\"content\"])\n",
    "    y_train.append(train_set[i][\"personality\"])\n",
    "\n",
    "word_test = []\n",
    "y_test = []\n",
    "for i in range(len(test_set)):\n",
    "    word_test.append(test_set[i][\"content\"])\n",
    "    y_test.append(test_set[i][\"personality\"])\n",
    "\n",
    "import re\n",
    "import copy\n",
    "import nltk\n",
    "print(word_train[0])\n",
    "s = re.split('[|,.\":!@ ]', word_train[0])\n",
    "l = []\n",
    "for i in s:\n",
    "    if i != '':\n",
    "        l.append(i)\n",
    "for j in range(len(l)):\n",
    "    l[j] = l[j].lower()\n",
    "print(l)\n",
    "\n",
    "word_train_split = copy.deepcopy(word_train)\n",
    "word_test_split = copy.deepcopy(word_test)\n",
    "def sentence_split(words):\n",
    "    for k in range(len(words)):\n",
    "        s = re.split('[|,.\":!@ ]', words[k])\n",
    "        l = []\n",
    "        for i in s:\n",
    "            if i != '':\n",
    "                l.append(i)\n",
    "        for j in range(len(l)):\n",
    "            l[j] = l[j].lower()\n",
    "        words[k] = l\n",
    "\n",
    "\n",
    "sentence_split(word_test_split)\n",
    "sentence_split(word_train_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.04709554e+00  1.46341324e+00 -1.10809493e+00 -8.62112492e-02\n",
      "  6.06594026e-01  4.90204722e-01  5.44232689e-02 -3.62945408e-01\n",
      "  7.15103075e-02  7.40818799e-01 -5.02325118e-01 -9.27270174e-01\n",
      " -3.42354804e-01  5.71782470e-01 -7.04888701e-02 -9.76805091e-01\n",
      "  1.05928016e+00  1.23611100e-01 -7.65622854e-01 -1.16724384e+00\n",
      " -1.08449897e-02 -6.76296175e-01 -2.88110524e-01  1.38453841e+00\n",
      " -1.41313696e+00  1.31390452e-01 -9.74839851e-02 -4.90896046e-01\n",
      " -1.30485260e+00 -2.36398205e-01 -1.03306934e-01 -3.00992906e-01\n",
      "  5.01909666e-02  9.09668729e-02  7.19120800e-01  2.15009972e-01\n",
      " -1.37476337e+00  6.18132055e-01 -1.88994870e-01 -4.38300341e-01\n",
      "  5.64552844e-01  1.52786568e-01 -6.25036359e-01 -2.45671958e-01\n",
      "  8.14079404e-01 -1.82049048e+00  2.79835075e-01  4.28210109e-01\n",
      "  4.08728063e-01 -3.21805149e-01 -3.92532498e-01 -2.22940877e-01\n",
      " -7.86253452e-01  2.37745070e+00 -1.04262829e+00  1.51021192e-02\n",
      "  3.67104709e-02 -2.05855027e-01 -3.29827219e-01 -1.06128109e+00\n",
      "  6.80127501e-01  1.00467372e+00  4.11224544e-01  2.56336361e-01\n",
      " -1.62429786e+00 -2.31006727e-01  3.16545218e-01 -7.03433633e-01\n",
      "  2.70994902e-01 -1.12526178e+00  8.12099159e-01 -3.39784436e-02\n",
      " -2.81944513e-01  1.23504031e+00  1.42953470e-01  1.59116769e+00\n",
      "  2.51155764e-01  1.10426784e+00 -7.27725983e-01 -1.32051185e-01\n",
      "  2.45245874e-01 -3.69216233e-01 -2.02065513e-01 -9.63389754e-01\n",
      "  1.17147245e-01  8.17226827e-01 -9.94448900e-01  1.95226833e-01\n",
      "  4.86231655e-01  8.60420346e-01 -1.33379537e-03  1.72068679e+00\n",
      " -7.66565859e-01  9.19832811e-02 -8.32922637e-01 -3.39758933e-01\n",
      " -7.50611007e-01  7.00886250e-01 -2.87975043e-01 -1.53648719e-01]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "word = word_train_split[0]\n",
    "documents = []\n",
    "c = 0\n",
    "for word in word_train_split:\n",
    "    documents.append(TaggedDocument(words=word, tags=[c]))\n",
    "    c += 1\n",
    "model = Doc2Vec(documents, vector_size=100, window=5, min_count=3, workers=4)\n",
    "model.train(documents, total_examples=model.corpus_count, epochs=20)\n",
    "print(model.infer_vector(word_train_split[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_train)):\n",
    "    l = [0, 0, 0, 0]\n",
    "    s = str(y_train[i])\n",
    "    if s[0] == 'I':\n",
    "        l[0] = 1\n",
    "    if s[1] == 'S':\n",
    "        l[1] = 1\n",
    "    if s[2] == 'T':\n",
    "        l[2] = 1\n",
    "    if s[3] == 'P':\n",
    "        l[3] = 1\n",
    "    y_train[i] = l\n",
    "y1_train = []\n",
    "y2_train = []\n",
    "y3_train = []\n",
    "y4_train = []\n",
    "for i in range(len(y_train)):\n",
    "    y1_train.append(y_train[i][0])\n",
    "    y2_train.append(y_train[i][1])\n",
    "    y3_train.append(y_train[i][2])\n",
    "    y4_train.append(y_train[i][3])\n",
    "for i in range(len(y_test)):\n",
    "    l = [0, 0, 0, 0]\n",
    "    s = str(y_test[i])\n",
    "    if s[0] == 'I':\n",
    "        l[0] = 1\n",
    "    if s[1] == 'S':\n",
    "        l[1] = 1\n",
    "    if s[2] == 'T':\n",
    "        l[2] = 1\n",
    "    if s[3] == 'P':\n",
    "        l[3] = 1\n",
    "    y_test[i] = l\n",
    "y1_test = []\n",
    "y2_test = []\n",
    "y3_test = []\n",
    "y4_test = []\n",
    "for i in range(len(y_test)):\n",
    "    y1_test.append(y_test[i][0])\n",
    "    y2_test.append(y_test[i][1])\n",
    "    y3_test.append(y_test[i][2])\n",
    "    y4_test.append(y_test[i][3])\n",
    "\n",
    "    \n",
    "x_train = []\n",
    "x_test = []\n",
    "for i in range(len(word_train_split)):\n",
    "    x_train.append(model.infer_vector(word_train_split[i]))\n",
    "for i in range(len(word_test_split)):\n",
    "    x_test.append(model.infer_vector(word_test_split[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "#nlp_model1 = svm.SVC(C=1.0, kernel='rbf', gamma='auto')\n",
    "nlp_model1 = LogisticRegression(C = 1.0, penalty = 'l2')\n",
    "#nlp_model1 = MLPClassifier(solver='lbfgs', alpha=0.01, hidden_layer_sizes=(16,), random_state=1, max_iter=10000)\n",
    "nlp_model1.fit(x_train, y1_train)\n",
    "y1_pred = nlp_model1.predict(x_test)\n",
    "nlp_model1.fit(x_train, y2_train)\n",
    "y2_pred = nlp_model1.predict(x_test)\n",
    "nlp_model1.fit(x_train, y3_train)\n",
    "y3_pred = nlp_model1.predict(x_test)\n",
    "nlp_model1.fit(x_train, y4_train)\n",
    "y4_pred = nlp_model1.predict(x_test)\n",
    "y_test = np.array(y_test)\n",
    "y_pred = np.zeros(y_test.shape)\n",
    "for i in range(len(y_test)):\n",
    "    y_pred[i][0] = y1_pred[i]\n",
    "    y_pred[i][1] = y2_pred[i]\n",
    "    y_pred[i][2] = y3_pred[i]\n",
    "    y_pred[i][3] = y4_pred[i]\n",
    "acc = 0\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i][0] == y_pred[i][0] and y_test[i][1] == y_pred[i][1] and y_test[i][2] == y_pred[i][2] and y_test[i][3] == y_pred[i][3]:\n",
    "        acc += 1\n",
    "acc = acc/(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23163659793814434\n"
     ]
    }
   ],
   "source": [
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
